---
title: "Advanced Statistical Modeling"
subtitle: "Non-parametric models - NIR data analyzed with splines"
author: "Haoran Mo, Alexandra Yamaui"
date: "December 28th, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# section 6.4 
```{r,message=FALSE}
load("meat.Rdata")
library(mgcv)
library(splines)
attach(meat)
abs.850.s <- sort(abs.850,index.return = TRUE)
lgfat <- log(Fat)
lgfat <- lgfat[abs.850.s$ix]
# point 1: combine lm and bs
k <- 3
```

```{r}
# This part tris to implement the 10-fold cross-validation
# t(): Matrix Transpose
seed <- 1800
set.seed(seed)
# step 1, find the relation between trainning error ~ df
train_set <- createDataPartition(lgfat, p = 0.8, list = FALSE)
train_df <- meat[train_set,]
test_df <- meat[-train_set,]
df <- 3:40
x <- sort(train_df$abs.850,index.return = TRUE)
y <- log(train_df$Fat)[x$ix]
x <- x$x

fitted_models_bs <- apply(t(df),2, function(degf) lm(y ~ bs(x =x, df = degf,degree = k)))
mse <- sapply(fitted_models_bs, function(obj) deviance(obj)/nobs(obj))

# step 2 10-fold
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = length(meat$Fat)))

cv_tmp <- matrix(NA, nrow = n_folds, ncol = length(df))
for (k in 1:n_folds) {
  # get 10 folds
  test_i <- which(folds_i == k)
  train_xy <- meat[-test_i, ]
  test_xy <- meat[test_i, ]
  x <- sort(train_xy$abs.850,index.return=TRUE)
  y <- log(train_xy$Fat)[x$ix]
  x <- x$x
  fitted_models <- apply(t(df), 2, function(degf) lm(y ~ bs(x =x, df = degf,degree = k)))
  x <- sort(test_xy$abs.850,index.return=TRUE)
  y <- log(test_xy$Fat)[x$ix]
  x <- x$x
  
  # randomly, some 'x' values may beyond boundary knots. So I set the seed.
  pred <- mapply(function(obj, degf) predict(obj, data.frame(bs(x, df = degf,degree = k))),fitted_models, df)
  cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y - y_hat)^2))
}
cv <- colMeans(cv_tmp)

# step 4 plot
require(Hmisc)

plot(df, mse, type = "l", lwd = 2, col = 3, ylab = "mse:green line; (y - y_hat): blue line", 
    xlab = "Flexibilty: df value", main = "10-fold Cross-Validation", ylim = c(0.2, 0.7))
cv_sd <- apply(cv_tmp, 2, sd)/sqrt(n_folds)
errbar(df, cv, cv + cv_sd, cv - cv_sd, add = TRUE, col = "steelblue2", pch = 19, lwd = 0.5)
lines(df, cv, lwd = 2, col = "steelblue2")
points(df, cv, col = "steelblue2", pch = 19)
# The green line is mse of trainning. As df increase, the model tend to be overfiting. The best df valuse shoud be around 10 from the viewing of bule line.
```


# section 6.4 point 2
```{r}
#point 1 combine lm and bs, with parameter df obtained from previous 10-fold cross-validation.
#basis <- bs(x =abs.850.s$x, knots = inner.knots,degree = k)
df_obtained <- 10
basis <- bs(x =abs.850.s$x, df=df_obtained,degree = k)
lm.spl <- lm(lgfat~basis)
plot(abs.850.s$x,lgfat)
lines(abs.850.s$x,lm.spl$fitted.values,col=2)

# point 2: smooth.spline
new.abs8 <- seq(min(abs.850.s$x),max(abs.850.s$x),length=length(abs.850.s$x))
sm.spl <- smooth.spline(abs.850.s$x,lgfat,df=df_obtained)
pred.lgfat <- predict(sm.spl,x = new.abs8)
lines(pred.lgfat,col=3)
# we can find that two lines match well.
```


